{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e0b9b70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import gensim\n",
    "\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP1: Generate Artificial Data\n",
    "\n",
    "We randomly generate input data:\n",
    "X1: (1000 [data size] x 2 [number of branches] x 3 [number of words for per branch input]) for Event-TIMEX classifier,\n",
    "X2: (1000 [data size] x 1 [number of branches] x 3 [number of words for per branch input]) for Event-DCT classifier.  \n",
    "\n",
    "with corresponding gold output Y:\n",
    "Y: (1000 [data size] x 2 [dimension of output tuples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 2, 3])\n",
      "torch.Size([500, 1, 3])\n",
      "torch.Size([500, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "class MultipleDatasets(Data.Dataset):\n",
    "    \"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Arguments:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, *tensors):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "dct_inputs = torch.LongTensor(500, 1, 3).random_(0, 100)\n",
    "time_inputs = torch.LongTensor(500, 1, 2, 3).random_(0, 100)\n",
    "\n",
    "targets = torch.Tensor(500, 3).random_(0, 2)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "# EPOCH_NUM = 100\n",
    "\n",
    "dataset = MultipleDatasets(dct_inputs, time_inputs, targets)\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "    dataset = dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 1,\n",
    ")\n",
    "\n",
    "print(time_inputs[:, :, :, :].size())\n",
    "print(time_inputs[:, :, 0, :].size())\n",
    "print(time_inputs[:, :, 1, :].size())\n",
    "\n",
    "# x = torch.randn(3, 4)\n",
    "# torch.index_select(x, 1, torch.tensor([0, 2]))\n",
    "\n",
    "# for epoch in range(EPOCH_NUM):\n",
    "#     for step, (batch_dct, batch_time, batch_y) in enumerate(loader):\n",
    "#         if step > 1:\n",
    "#             continue\n",
    "#         print('Epoch:', epoch, '| Step:', step, '| batch dct:', batch_dct.numpy(),  '| batch time:',\n",
    "#               batch_time.numpy(), '|batch y:', batch_y.numpy())\n",
    "\n",
    "# k=1\n",
    "\n",
    "# for dct, time, target in zip(dct_inputs[:k], time_inputs[:k], targets[:k]):\n",
    "#     print(dct)\n",
    "#     print(time)\n",
    "#     print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040683\n"
     ]
    }
   ],
   "source": [
    "word2ix = {}\n",
    "pre_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/fei-c/Resources/embed/giga-aacw.d200.bin', binary=True)\n",
    "for word, value in pre_model.vocab.items():\n",
    "    word2ix[word] = value.index\n",
    "print(len(word2ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1040683, 200])\n",
      "torch.Size([200])\n",
      "tensor(1.00000e-03 *\n",
      "       [ 2.0010,  2.2100, -1.9150, -1.6390,  0.6830,  1.5110])\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = torch.FloatTensor(pre_model.vectors)\n",
    "print(pretrained_weights.size())\n",
    "pretrained_embed = nn.Embedding.from_pretrained(pretrained_weights, freeze=True)\n",
    "input = torch.LongTensor([[0, ]])\n",
    "print(pretrained_embed(input).squeeze().size())\n",
    "print(pretrained_embed(input).squeeze()[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-04fe090c2600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPS_THRES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m update_strategies = torch.tensor([[0, 0, 0],\n\u001b[0m\u001b[1;32m      5\u001b[0m                              \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                              \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import random\n",
    "EPS_THRES = 0\n",
    "\n",
    "update_strategies = torch.tensor([[0, 0, 0],\n",
    "                             [1, 0, 0],\n",
    "                             [0, 1, 0],\n",
    "                             [0, 0, 1],\n",
    "                             [1, 1, 0],\n",
    "                             [0, 1, 1],\n",
    "                             [1, 0, 1],\n",
    "                             [1, 1, 1]], dtype=torch.int16)\n",
    "\n",
    "\n",
    "def get_index_of_max(scores):\n",
    "    index = scores.max(0)[1].item()\n",
    "    return index\n",
    "\n",
    "ACTION_TO_IX = {'COL0':0, 'COL1':1, 'COL2':2, 'COL01':3, 'COL12':4, 'COL012':5, 'NONE':6}\n",
    "IX_TO_ACTION = {v: k for k, v in ACTION_TO_IX.items()}\n",
    "\n",
    "def select_action(out_score, IX_TO_ACTION):\n",
    "    sample = random.random()\n",
    "    if sample > EPS_THRES:\n",
    "        index = get_index_of_max(out_score)\n",
    "        return IX_TO_ACTION[index]\n",
    "    else:\n",
    "        return IX_TO_ACTION[random.randrange(7)]\n",
    "    \n",
    "\n",
    "def action2out(action, curr_out, curr_time, IX_TO_ACTION):\n",
    "    \n",
    "    def update_col0(curr_out, curr_time):\n",
    "        curr_out[0] = curr_time\n",
    "        return curr_out\n",
    "    \n",
    "    def update_col1(curr_out, curr_time):\n",
    "        curr_out[1] = curr_time\n",
    "        return curr_out\n",
    "    \n",
    "    def update_col2(curr_out, curr_time):\n",
    "        curr_out[2] = curr_time\n",
    "        return curr_out\n",
    "    \n",
    "    def update_col01(curr_out, curr_time):\n",
    "        curr_out[0] = curr_time\n",
    "        curr_out[1] = curr_time\n",
    "        return curr_out\n",
    "        \n",
    "    def update_col12(curr_out, curr_time):\n",
    "        curr_out[1] = curr_time\n",
    "        curr_out[2] = curr_time\n",
    "        return curr_out\n",
    "    \n",
    "    def update_col012(curr_out, curr_time):\n",
    "        curr_out[0] = curr_time\n",
    "        curr_out[1] = curr_time\n",
    "        curr_out[2] = curr_time\n",
    "        return curr_out\n",
    "    \n",
    "    def update_none(curr_out, curr_time):\n",
    "        return curr_out\n",
    "    \n",
    "    if action == 'COL0':\n",
    "        return update_col0(curr_out, curr_time)\n",
    "    elif action == 'COL1':\n",
    "        return update_col1(curr_out, curr_time)\n",
    "    elif action == 'COL2':\n",
    "        return update_col2(curr_out, curr_time)\n",
    "    elif action == 'COL01':\n",
    "        return update_col01(curr_out, curr_time)\n",
    "    elif action == 'COL12':\n",
    "        return update_col12(curr_out, curr_time)\n",
    "    elif action == 'COL012':\n",
    "        return update_col012(curr_out, curr_time)\n",
    "    elif action == 'NONE':\n",
    "        return update_none(curr_out, curr_time)\n",
    "    else:\n",
    "        raise Expception(\"[ERROR]Wrong action!!!\")\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "def batch_action2out(out_scores, norm_times, IX_TO_ACTION, BATCH_SIZE):\n",
    "    preds_out = torch.ones(BATCH_SIZE, 3) * -1 ## initial prediction\n",
    "    for i in range(out_scores.size()[0]):\n",
    "        for j in range(out_scores.size()[1]):\n",
    "            action = select_action(out_scores[i][j], IX_TO_ACTION)\n",
    "#             print(action)\n",
    "            action2out(action, preds_out[i], 0 if i == 0 else 1, IX_TO_ACTION)\n",
    "    return preds_out\n",
    "            \n",
    "        \n",
    "# class diff_elem_loss(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(diff_elem_loss, self).__init__()\n",
    "    \n",
    "#     def forward(self, pred_out, target):\n",
    "#         diff_t = torch.eq(pred_out, target)\n",
    "#         sum_elem = pred_out.numel()\n",
    "#         loss = ( sum_elem - diff_t.sum().item() ) / sum_elem\n",
    "#         return autograd.Variable(torch.FloatTensor([loss]), requires_grad=True)\n",
    "\n",
    "    \n",
    "# def diff_elem_loss(pred_out, target)\n",
    "#     diff_t = torch.eq(pred_out, target)\n",
    "#     sum_elem = pred_out.nelement()\n",
    "#     loss = ( sum_elem - diff_t.sum().item() ) / sum_elem\n",
    "#     return autograd.Variable(torch.FloatTensor([loss]), requires_grad=True)\n",
    "    \n",
    "class distance_loss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(distance_loss, self).__init__()\n",
    "    \n",
    "    def forward(self, action_out, target):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCTDetector(nn.Module):\n",
    "    def __init__(self, word_embeddings, embedding_dim, dct_hidden_dim, action_size, batch_size):\n",
    "        super(DCTDetector, self).__init__()\n",
    "        ## initialize parameters\n",
    "        self.dct_hidden_dim = dct_hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        ## initialize neural layers\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        self.dct_tagger = nn.LSTM(embedding_dim, dct_hidden_dim // 2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.tagger_dropout = nn.Dropout(p=0.5)\n",
    "        self.dct_hidden2action = nn.Linear(dct_hidden_dim, action_size)\n",
    "        self.dct_hidden = self.init_dct_hidden()\n",
    "        \n",
    "    def init_dct_hidden(self):\n",
    "        return (torch.zeros(2, self.batch_size, self.dct_hidden_dim // 2),\n",
    "                torch.zeros(2, self.batch_size, self.dct_hidden_dim // 2))\n",
    "          \n",
    "    def forward(self, dct_input):\n",
    "        dct_var = dct_input.view(self.batch_size, -1)\n",
    "        dct_embeds = self.word_embeddings(dct_var).view(self.batch_size, dct_input.size()[-1], -1)\n",
    "        dct_embeds = self.embedding_dropout(dct_embeds)\n",
    "#         print(dct_embeds.size())\n",
    "        dct_out, self.dct_hidden = self.dct_tagger(dct_embeds, self.dct_hidden)\n",
    "        dct_score = self.dct_hidden2action(dct_out[:, -1, :])\n",
    "        return dct_score\n",
    "        \n",
    "class TimeDetector(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_embeddings, embedding_dim, time_hidden_dim, action_size, batch_size):\n",
    "        super(TimeDetector, self).__init__()\n",
    "        ## initialize parameters\n",
    "        self.time_hidden_dim = time_hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        ## initialize neural layers\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.left_embedding_dropout = nn.Dropout(p=0.5)\n",
    "        self.right_embedding_dropout = nn.Dropout(p=0.5)\n",
    "        self.left_tagger = nn.LSTM(embedding_dim, time_hidden_dim // 2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.left_tagger_dropout = nn.Dropout(p=0.5)\n",
    "        self.right_tagger = nn.LSTM(embedding_dim, time_hidden_dim // 2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.right_tagger_dropout = nn.Dropout(p=0.5)\n",
    "        self.time_hidden2action = nn.Linear(time_hidden_dim * 2, action_size)\n",
    "        self.left_hidden = self.init_left_time_hidden()\n",
    "        self.right_hidden = self.init_right_time_hidden()\n",
    "        \n",
    "    def init_left_time_hidden(self):\n",
    "        return (torch.zeros(2, self.batch_size, self.time_hidden_dim // 2),\n",
    "                torch.zeros(2, self.batch_size, self.time_hidden_dim // 2))\n",
    "    \n",
    "    def init_right_time_hidden(self):\n",
    "        return (torch.zeros(2, self.batch_size, self.time_hidden_dim // 2),\n",
    "                torch.zeros(2, self.batch_size, self.time_hidden_dim // 2))\n",
    "    \n",
    "    def forward(self, left_input, right_input):\n",
    "        \n",
    "        ## left branch\n",
    "        left_var = left_input\n",
    "        left_embeds = self.word_embeddings(left_var).view(self.batch_size, left_input.size()[-1], -1)\n",
    "        left_embeds = self.left_embedding_dropout(left_embeds)\n",
    "        left_out, self.left_hidden = self.left_tagger(left_embeds, self.left_hidden)\n",
    "        left_out = self.left_tagger_dropout(left_out[:,-1, :])\n",
    "\n",
    "        ## right branch\n",
    "        right_var = right_input\n",
    "        right_embeds = self.word_embeddings(right_var).view(self.batch_size, right_input.size()[-1], -1)\n",
    "        right_embeds = self.right_embedding_dropout(right_embeds)\n",
    "        right_out, self.right_hidden = self.right_tagger(right_embeds, self.right_hidden)\n",
    "        right_out = self.right_tagger_dropout(right_out[:,-1, :])\n",
    "\n",
    "        ## concatenation\n",
    "\n",
    "        time_out = torch.cat((left_out, right_out), 1)\n",
    "        time_score = self.time_hidden2action(time_out)\n",
    "        return time_score\n",
    "                                       \n",
    "class TimeInferrer(nn.Module):\n",
    "                                       \n",
    "    def __init__(self, embedding_dim, dct_hidden_dim, time_hidden_dim, vocab_size, action_size, batch_size):\n",
    "        super(TimeInferrer, self).__init__()\n",
    "        self.dct_hidden_dim = dct_hidden_dim\n",
    "        self.time_hidden_dim = time_hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim                              \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dct_detector = DCTDetector(self.word_embeddings, \n",
    "                                        self.embedding_dim, \n",
    "                                        self.dct_hidden_dim, \n",
    "                                        action_size, \n",
    "                                        batch_size)\n",
    "        self.time_detector = TimeDetector(self.word_embeddings, \n",
    "                                          self.embedding_dim, \n",
    "                                          self.time_hidden_dim, \n",
    "                                          action_size, \n",
    "                                          batch_size)\n",
    "                                       \n",
    "    def forward(self, dct_input, time_inputs):\n",
    "        \n",
    "        ## event to dct\n",
    "        dct_score = self.dct_detector(dct_input)\n",
    "        out_scores = dct_score.clone().view(self.batch_size, 1, -1)\n",
    "#         print(out_scores.size())\n",
    "        \n",
    "        ## event to per time expression\n",
    "        for time_index in range(time_inputs.size()[1]):\n",
    "            left_input = time_inputs[:, time_index, 0, :]\n",
    "            right_input = time_inputs[:, time_index, 1, :]\n",
    "            time_score = self.time_detector(left_input, right_input).view(self.batch_size, 1, -1)\n",
    "#             print(time_score.size())\n",
    "            out_scores = torch.cat((out_scores, time_score), dim=1)\n",
    "#             print(out_scores.size())\n",
    "            \n",
    "        \n",
    "        return out_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeInferrer(\n",
      "  (word_embeddings): Embedding(100, 64)\n",
      "  (dct_detector): DCTDetector(\n",
      "    (word_embeddings): Embedding(100, 64)\n",
      "    (embedding_dropout): Dropout(p=0.5)\n",
      "    (dct_tagger): LSTM(64, 30, batch_first=True, bidirectional=True)\n",
      "    (tagger_dropout): Dropout(p=0.5)\n",
      "    (dct_hidden2action): Linear(in_features=60, out_features=8, bias=True)\n",
      "  )\n",
      "  (time_detector): TimeDetector(\n",
      "    (word_embeddings): Embedding(100, 64)\n",
      "    (left_embedding_dropout): Dropout(p=0.5)\n",
      "    (right_embedding_dropout): Dropout(p=0.5)\n",
      "    (left_tagger): LSTM(64, 25, batch_first=True, bidirectional=True)\n",
      "    (left_tagger_dropout): Dropout(p=0.5)\n",
      "    (right_tagger): LSTM(64, 25, batch_first=True, bidirectional=True)\n",
      "    (right_tagger_dropout): Dropout(p=0.5)\n",
      "    (time_hidden2action): Linear(in_features=100, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "* word_embeddings.weight\n",
      "* dct_detector.dct_tagger.weight_ih_l0\n",
      "* dct_detector.dct_tagger.weight_hh_l0\n",
      "* dct_detector.dct_tagger.bias_ih_l0\n",
      "* dct_detector.dct_tagger.bias_hh_l0\n",
      "* dct_detector.dct_tagger.weight_ih_l0_reverse\n",
      "* dct_detector.dct_tagger.weight_hh_l0_reverse\n",
      "* dct_detector.dct_tagger.bias_ih_l0_reverse\n",
      "* dct_detector.dct_tagger.bias_hh_l0_reverse\n",
      "* dct_detector.dct_hidden2action.weight\n",
      "* dct_detector.dct_hidden2action.bias\n",
      "* time_detector.left_tagger.weight_ih_l0\n",
      "* time_detector.left_tagger.weight_hh_l0\n",
      "* time_detector.left_tagger.bias_ih_l0\n",
      "* time_detector.left_tagger.bias_hh_l0\n",
      "* time_detector.left_tagger.weight_ih_l0_reverse\n",
      "* time_detector.left_tagger.weight_hh_l0_reverse\n",
      "* time_detector.left_tagger.bias_ih_l0_reverse\n",
      "* time_detector.left_tagger.bias_hh_l0_reverse\n",
      "* time_detector.right_tagger.weight_ih_l0\n",
      "* time_detector.right_tagger.weight_hh_l0\n",
      "* time_detector.right_tagger.bias_ih_l0\n",
      "* time_detector.right_tagger.bias_hh_l0\n",
      "* time_detector.right_tagger.weight_ih_l0_reverse\n",
      "* time_detector.right_tagger.weight_hh_l0_reverse\n",
      "* time_detector.right_tagger.bias_ih_l0_reverse\n",
      "* time_detector.right_tagger.bias_hh_l0_reverse\n",
      "* time_detector.time_hidden2action.weight\n",
      "* time_detector.time_hidden2action.bias\n",
      "None\n",
      "Epoch 0 Step 9 , Epoch loss: 0.6393 True\n",
      "None\n",
      "Epoch 1 Step 9 , Epoch loss: 0.6307 True\n",
      "None\n",
      "Epoch 2 Step 9 , Epoch loss: 0.6360 True\n",
      "None\n",
      "Epoch 3 Step 9 , Epoch loss: 0.6300 True\n",
      "None\n",
      "Epoch 4 Step 9 , Epoch loss: 0.6120 True\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "DCT_HIDDEN_DIM = 60\n",
    "TIME_HIDDEN_DIM = 50\n",
    "VOCAB_SIZE = 100\n",
    "ACTION_SIZE = 8\n",
    "EPOCH_NUM = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "def score2pred_2(out_scores, norm_times, BATCH_SIZE, update_strategies):\n",
    "    norm_times = torch.tensor([[0, 0, 0], [1, 1, 1]])\n",
    "    preds_out = torch.ones(BATCH_SIZE, 3) * -1\n",
    "    preds_out.requires_grad_()\n",
    "    class_ids = torch.argmax(out_scores, dim=2)\n",
    "    for i in range(out_scores.size()[0]):\n",
    "        for cid, time in zip(class_ids[i], norm_times):\n",
    "            preds_out[i] = update_strategies[cid].float() * time.float() + (torch.ones_like(update_strategies[cid]) - update_strategies[cid]).float() * preds_out[i].float()\n",
    "    return preds_out\n",
    "\n",
    "def diff_elem_loss(pred_out, target):\n",
    "    diff_t = torch.eq(pred_out, target)\n",
    "    loss = torch.div((pred_out.numel() - diff_t.sum().float()), pred_out.numel())\n",
    "    loss.requires_grad_()\n",
    "    return loss\n",
    "\n",
    "model = TimeInferrer(EMBEDDING_DIM, DCT_HIDDEN_DIM, TIME_HIDDEN_DIM, VOCAB_SIZE, ACTION_SIZE, BATCH_SIZE)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(name)\n",
    "    else:\n",
    "        print('*', name)\n",
    "## train model\n",
    "a = list(model.parameters())[1]\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "   \n",
    "    total_loss = torch.Tensor([0])\n",
    "    for step, (dct_input, time_input, target) in enumerate(loader):\n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        time_scores = model(dct_input, time_input)\n",
    "#         print(time_scores.requires_grad)\n",
    "        pred_out = score2pred_2(time_scores, None, BATCH_SIZE, update_strategies)\n",
    "#         print(pred_out.requires_grad)\n",
    "        loss = diff_elem_loss(pred_out, target)\n",
    "        \n",
    "#         print(loss)\n",
    "#         print(list(model.parameters())[0].grad)\n",
    "#         print(loss)\n",
    "#         print(loss.grad)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ## check if the model parameters being updated\n",
    "\n",
    "#         print(len(model.word_embeddings.weight[1]))\n",
    "#         print(list(model.parameters())[0].grad)\n",
    "        \n",
    "        \n",
    "        total_loss += loss.data.item() * pred_out.size()[0]\n",
    "#         print('')\n",
    "    for name, param in model.named_parameters():\n",
    "        if name == 'time_detector.time_hidden2action.weight':\n",
    "            print(param.grad)\n",
    "    b = list(model.parameters())[1]\n",
    "    print('Epoch', epoch, 'Step', step, ', Epoch loss: %.4f' % (total_loss / 500), torch.equal(a.data, b.data))\n",
    "#     print(model.word_embeddings.data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeInferrer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, dct_hidden_dim, time_hidden_dim, vocab_size, action_size, batch_size):\n",
    "        super(TimeInferrer, self).__init__()\n",
    "        self.dct_hidden_dim = dct_hidden_dim\n",
    "        self.time_hidden_dim = time_hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        ## DCT and timex relation tagger\n",
    "        self.dct_tagger = nn.LSTM(embedding_dim, dct_hidden_dim // 2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.left_time_tagger = nn.LSTM(embedding_dim, time_hidden_dim // 2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.right_time_tagger = nn.LSTM(embedding_dim, time_hidden_dim // 2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        ## The linear layer that maps from LSTM output to action space\n",
    "        self.dct_hidden2action = nn.Linear(dct_hidden_dim, action_size)\n",
    "        self.time_hidden2action = nn.Linear(time_hidden_dim * 2, action_size)\n",
    "        \n",
    "        self.dct_hidden = self.init_dct_hidden()\n",
    "#         self.left_time_hidden = self.init_left_time_hidden()\n",
    "#         self.right_time_hidden = self.init_right_time_hidden()\n",
    "        \n",
    "    def init_dct_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(2, self.batch_size, self.dct_hidden_dim // 2)),\n",
    "                autograd.Variable(torch.zeros(2, self.batch_size, self.dct_hidden_dim // 2)))\n",
    "    \n",
    "    def init_left_time_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(2, self.batch_size, self.time_hidden_dim // 2)),\n",
    "                autograd.Variable(torch.zeros(2, self.batch_size, self.time_hidden_dim // 2)))\n",
    "    \n",
    "    def init_right_time_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(2, self.batch_size, self.time_hidden_dim // 2)),\n",
    "                autograd.Variable(torch.zeros(2, self.batch_size, self.time_hidden_dim // 2)))\n",
    "    \n",
    "    def forward(self, dct_in, time_inputs):\n",
    "        \n",
    "#         print(dct_in.size(), time_inputs.size())\n",
    "        ## store all the dct_score and time_score into one list for calculating loss\n",
    "#         print(self.word_embeddings.weight.data[48])\n",
    "        \n",
    "        ## event to dct\n",
    "        dct_var = autograd.Variable(dct_in.view(self.batch_size, -1))\n",
    "        dct_embeds = self.word_embeddings(dct_var).view(self.batch_size, dct_in.size()[-1], -1)\n",
    "        dct_out, self.dct_hidden = self.dct_tagger(dct_embeds, self.dct_hidden)\n",
    "        dct_score = self.dct_hidden2action(dct_out[:, -1, :])\n",
    "        time_scores = dct_score.clone().view(self.batch_size, 1, -1)\n",
    "        \n",
    "        ## event to per time expression\n",
    "        for time_index in range(time_inputs.size()[1]):\n",
    "            \n",
    "            \n",
    "            self.left_time_hidden = self.init_left_time_hidden()\n",
    "            self.right_time_hidden = self.init_right_time_hidden()\n",
    "            \n",
    "            ## left branch\n",
    "            left_time = time_inputs[:, time_index, 0, :]\n",
    "            left_time_var = autograd.Variable(left_time)\n",
    "            left_time_embeds = self.word_embeddings(left_time_var).view(self.batch_size, left_time.size()[-1], -1)\n",
    "            left_time_out, self.left_time_hidden = self.left_time_tagger(left_time_embeds, self.left_time_hidden)\n",
    "            \n",
    "            ## right branch\n",
    "            right_time = time_inputs[:, time_index, 1, :]\n",
    "            right_time_var = autograd.Variable(right_time)\n",
    "            right_time_embeds = self.word_embeddings(right_time_var).view(self.batch_size, right_time.size()[-1], -1)\n",
    "            right_time_out, self.right_time_hidden = self.right_time_tagger(right_time_embeds, self.right_time_hidden)\n",
    "            \n",
    "            ## concatenation\n",
    "            \n",
    "            time_out = torch.cat((left_time_out[:,-1, :], right_time_out[:,-1, :]), 1)\n",
    "            time_score = self.time_hidden2action(time_out)\n",
    "            time_scores = torch.cat((time_scores, time_score.view(self.batch_size, 1, -1)), 1)\n",
    "        \n",
    "        \n",
    "        return time_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

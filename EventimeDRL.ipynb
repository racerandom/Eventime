{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import random\n",
    "import time as timer\n",
    "import pdb\n",
    "import gensim\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# import eventime as Tlink\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 3]) torch.Size([50, 1, 2, 3]) torch.Size([50, 3])\nFalse\n"
     ]
    }
   ],
   "source": [
    "dct_inputs = torch.randint(0, 100, (50, 1, 3), dtype=torch.long)\n",
    "time_inputs = torch.randint(0, 100, (50, 1, 2, 3), dtype=torch.long)\n",
    "targets = torch.randint(0, 2, (50, 3), dtype=torch.long)\n",
    "print(dct_inputs.size(), time_inputs.size(), targets.size())\n",
    "print(dct_inputs.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Tlink.MultipleDatasets(dct_inputs, time_inputs, targets)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "loader = Data.DataLoader(\n",
    "    dataset = dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 1,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "EMBEDDING_DIM = 64\n",
    "DCT_HIDDEN_DIM = 60\n",
    "TIME_HIDDEN_DIM = 50\n",
    "VOCAB_SIZE = 100\n",
    "ACTION_SIZE = 8\n",
    "EPOCH_NUM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state_i, dpath_input):\n",
    "    probs = policy(state_i, dpath_input)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + GAMMA * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.tensor(rewards, device=device)\n",
    "    # print(rewards, eps, rewards.mean(), rewards.std())\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps) # reward normalization\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        # print(log_prob, reward, -log_prob * reward)\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    # print('finish:', policy.rewards, policy.saved_log_probs, torch.cat(policy_loss))\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state_i, action, anchor, target):\n",
    "\n",
    "    update_strategies = torch.tensor([[0, 0, 0],\n",
    "                                      [1, 0, 0],\n",
    "                                      [0, 1, 0],\n",
    "                                      [0, 0, 1],\n",
    "                                      [1, 1, 0],\n",
    "                                      [0, 1, 1],\n",
    "                                      [1, 0, 1],\n",
    "                                      [1, 1, 1]], dtype=torch.long, device=device)\n",
    "    timex = torch.tensor([0, 0, 0], dtype=torch.long, device=device) \\\n",
    "        if state_i == 0 else torch.tensor([1, 1, 1], dtype=torch.long, device=device)\n",
    "    anchor = update_strategies[action] * timex \\\n",
    "             + (torch.ones_like(update_strategies[action], dtype=torch.long, device=device) - update_strategies[action]) \\\n",
    "             * anchor\n",
    "    # print(anchor, target)\n",
    "    # print(torch.eq(anchor, target).sum(), anchor.numel())\n",
    "    reward = torch.div(torch.eq(anchor, target).sum().float(), anchor.numel())\n",
    "    state_i += 1\n",
    "    return state_i, reward, anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 , loss: 0.0113 , reward: 0.3467 , 3.32 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 , loss: -0.1033 , reward: 0.3533 , 10.7 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2 , loss: -0.1803 , reward: 0.4133 , 15.6 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3 , loss: -0.1843 , reward: 0.4733 , 21.1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4 , loss: -0.0857 , reward: 0.3667 , 26.8 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5 , loss: -0.0588 , reward: 0.4133 , 33.1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 6 , loss: -0.1993 , reward: 0.3800 , 39.8 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 7 , loss: -0.1919 , reward: 0.4267 , 47.1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 8 , loss: -0.1926 , reward: 0.4667 , 52.0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 9 , loss: -0.1702 , reward: 0.4333 , 58.9 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10 , loss: -0.2217 , reward: 0.3867 , 73.7 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 11 , loss: -0.0888 , reward: 0.3733 , 79.6 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 12 , loss: -0.2986 , reward: 0.4400 , 91.7 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 13 , loss: -0.2864 , reward: 0.5000 , 103. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 14 , loss: -0.3266 , reward: 0.4067 , 124. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 15 , loss: -0.4152 , reward: 0.4467 , 166. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 16 , loss: -0.2532 , reward: 0.3333 , 131. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 17 , loss: -0.2882 , reward: 0.4533 , 132. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 18 , loss: -0.4488 , reward: 0.4800 , 120. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 19 , loss: -0.4537 , reward: 0.4467 , 132. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 20 , loss: -0.1848 , reward: 0.4067 , 125. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 21 , loss: -0.2145 , reward: 0.3933 , 133. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 22 , loss: -0.3986 , reward: 0.4267 , 147. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 23 , loss: -0.1957 , reward: 0.5000 , 158. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 24 , loss: -0.3219 , reward: 0.4533 , 162. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 25 , loss: -0.2891 , reward: 0.4400 , 169. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 26 , loss: -0.1917 , reward: 0.4133 , 206. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 27 , loss: -0.2425 , reward: 0.4133 , 208. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 28 , loss: -0.2045 , reward: 0.4000 , 208. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 29 , loss: -0.2265 , reward: 0.4267 , 216. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 30 , loss: -0.3524 , reward: 0.3867 , 221. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 31 , loss: -0.2425 , reward: 0.3800 , 235. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 32 , loss: -0.4200 , reward: 0.4867 , 231. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 33 , loss: -0.2184 , reward: 0.4533 , 240. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 34 , loss: -0.2513 , reward: 0.4133 , 249. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 35 , loss: -0.4097 , reward: 0.4400 , 257. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 36 , loss: -0.2778 , reward: 0.4333 , 268. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 37 , loss: -0.5463 , reward: 0.4200 , 257. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 38 , loss: -0.3539 , reward: 0.4733 , 282. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 39 , loss: -0.2312 , reward: 0.4067 , 287. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 40 , loss: -0.2554 , reward: 0.4133 , 299. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 41 , loss: -0.3933 , reward: 0.4200 , 294. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 42 , loss: -0.1829 , reward: 0.4467 , 300. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 43 , loss: -0.1371 , reward: 0.4133 , 308. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 44 , loss: -0.2186 , reward: 0.4533 , 317. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 45 , loss: -0.5117 , reward: 0.4667 , 327. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 46 , loss: -0.3183 , reward: 0.5200 , 353. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 47 , loss: -0.3742 , reward: 0.4667 , 357. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 48 , loss: -0.5219 , reward: 0.4467 , 361. seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 49 , loss: -0.3861 , reward: 0.4800 , 370. seconds\n"
     ]
    }
   ],
   "source": [
    "policy = Tlink.DqnInferrer(EMBEDDING_DIM, DCT_HIDDEN_DIM, TIME_HIDDEN_DIM, VOCAB_SIZE, ACTION_SIZE, BATCH_SIZE).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    start_time = timer.time()\n",
    "    total_loss = torch.tensor([0.], device=device)\n",
    "    total_reward = torch.tensor([0.], device=device)\n",
    "    \n",
    "    for episode, (dct_input, time_inputs, target) in enumerate(loader):\n",
    "#         print(dct_input, time_inputs, target)\n",
    "        dct_input = dct_input.to(device=device)\n",
    "        time_inputs = time_inputs.to(device=device)\n",
    "        target = target.to(device=device)\n",
    "        \n",
    "        ## dct state\n",
    "        state_i = 0\n",
    "        dct_action = select_action(state_i, dct_input)\n",
    "        anchor = torch.tensor([-1, -1, -1], dtype=torch.long, device=device)\n",
    "        state_i, reward, anchor = step(state_i, dct_action, anchor, target)\n",
    "#         print('dct action:', dct_action, anchor, reward)\n",
    "        policy.rewards.append(reward)\n",
    "\n",
    "        ## time states\n",
    "        for timex in range(time_inputs.size()[1]):\n",
    "            time_input = time_inputs[:, timex, :, :]\n",
    "            time_action = select_action(state_i, time_input)\n",
    "            state_i, reward, anchor = step(state_i, time_action, anchor, target)\n",
    "#             print('time action:', time_action, anchor, reward)\n",
    "            policy.rewards.append(reward)\n",
    "        total_reward += reward\n",
    "#         print(total_reward)\n",
    "        total_loss += finish_episode()\n",
    "    print('Iter:', epoch, ', loss: %.4f' % (total_loss.item()/50), ', reward: %.4f' % (total_reward.item() / 50), \", %.4s seconds\" % (timer.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
